{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "### Production ML pipeline in Azure AI - Machine Learning Studio for Credit Card Defaults model with Python SDK v2 & MLFlow\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Authentication to Azure"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle to the workspace\n",
        "from azure.ai.ml import MLClient\n",
        "\n",
        "# Authentication package\n",
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "\n",
        "try:\n",
        "    # Attempt to create a credential using DefaultAzureCredential\n",
        "    credential = DefaultAzureCredential()\n",
        "    \n",
        "    # Check if the credential can successfully get a token for Azure Resource Manager\n",
        "    credential.get_token(\"https://management.azure.com/.default\")\n",
        "    \n",
        "except Exception as ex:\n",
        "    # If DefaultAzureCredential fails, handle the exception\n",
        "    \n",
        "    # Fall back to InteractiveBrowserCredential for interactive sign-in\n",
        "    credential = InteractiveBrowserCredential()\n"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "import-mlclient",
        "gather": {
          "logged": 1697899278194
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Access the Azure workspace"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a handle to the workspace\n",
        "ml_client = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id=\"...\",\n",
        "    resource_group_name=\"rg-mlops\",\n",
        "    workspace_name=\"azureml-WS05\",\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "ml_client",
        "gather": {
          "logged": 1697899278882
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import Data\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "web_path = \"https://azuremlexamples.blob.core.windows.net/datasets/credit_card/default%20of%20credit%20card%20clients.csv\"\n",
        "\n",
        "credit_data = Data(\n",
        "    name=\"creditcard_blob_csv_defaults\",\n",
        "    path=web_path,\n",
        "    type=AssetTypes.URI_FILE,\n",
        "    description=\"Dataset for credit card defaults\",\n",
        "    tags={\"source_type\": \"web\", \"source\": \"azure_ml_creditcard_default_data\"},\n",
        "    version=\"1.0.0\",\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "name": "credit_data",
        "gather": {
          "logged": 1697899278981
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a data asset"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create or update a dataset in the Azure Machine Learning workspace\n",
        "credit_data = ml_client.data.create_or_update(credit_data)\n",
        "\n",
        "# Provide feedback to the user about the operation result\n",
        "print(\n",
        "    f\"Dataset with name {credit_data.name} was registered to workspace, the dataset version is {credit_data.version}\"\n",
        ")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Dataset with name creditcard_blob_csv_defaults was registered to workspace, the dataset version is 1.0.0\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "update-credit_data",
        "gather": {
          "logged": 1697899279910
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create an environment"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the 'os' module, which provides operating system-related functions.\n",
        "import os\n",
        "\n",
        "# Define a directory path for dependencies, in this case, \"./dependencies.\"\n",
        "dependencies_dir = \"./dependencies\"\n",
        "\n",
        "# Create a directory at the specified path if it doesn't already exist (exist_ok=True).\n",
        "os.makedirs(dependencies_dir, exist_ok=True)\n"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "name": "dependencies_dir",
        "gather": {
          "logged": 1697899280011
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Create the file in the dependencies directory."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {dependencies_dir}/conda.yaml\n",
        "name: model-env\n",
        "channels:\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.8\n",
        "  - numpy=1.21.2\n",
        "  - pip=21.2.4\n",
        "  - scikit-learn=0.24.2\n",
        "  - scipy=1.7.1\n",
        "  - pandas>=1.1,<1.2\n",
        "  - pip:\n",
        "    - inference-schema[numpy-support]==1.3.0\n",
        "    - xlrd==2.0.1\n",
        "    - azureml-mlflow==1.42.0"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./dependencies/conda.yaml\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "conda.yaml"
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Use the yaml file to create and register this custom environment in the workspace"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the 'Environment' class from the Azure Machine Learning SDK\n",
        "from azure.ai.ml.entities import Environment\n",
        "\n",
        "# Define a custom environment name\n",
        "custom_env_name = \"creditcard-default-scikit-learn-env\"\n",
        "\n",
        "# Create an 'Environment' object to define the custom environment\n",
        "pipeline_job_env = Environment(\n",
        "    name=custom_env_name,\n",
        "    description=\"Custom environment for Credit Card Defaults pipeline\",\n",
        "    tags={\"scikit-learn\": \"0.24.2\"},\n",
        "    conda_file=os.path.join(dependencies_dir, \"conda.yaml\"),\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
        "    version=\"0.1.1\",\n",
        ")\n",
        "\n",
        "# Create or update the custom environment in the Azure Machine Learning workspace\n",
        "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
        "\n",
        "# Provide user feedback about the environment registration\n",
        "print(\n",
        "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
        ")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Environment with name creditcard-default-scikit-learn-env is registered to workspace, the environment version is 0.1.1\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "custom_env_name",
        "gather": {
          "logged": 1697899281551
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The training pipeline\n",
        "\n",
        "\n",
        "\n",
        "## Create component 1: data prep (using programmatic definition)\n",
        "\n",
        "This component handles the preprocessing of the data. The preprocessing task is performed in the *data_prep.py* python file.\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the 'os' module, which provides operating system-related functions.\n",
        "import os\n",
        "\n",
        "# Define a directory path for data preparation source code, in this case, \"./components/data_prep.\"\n",
        "data_prep_src_dir = \"./components/data_prep\"\n",
        "\n",
        "# Create a directory at the specified path if it doesn't already exist (exist_ok=True).\n",
        "os.makedirs(data_prep_src_dir, exist_ok=True)\n"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "data_prep_src_dir",
        "gather": {
          "logged": 1697899281655
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "This script performs the simple task of splitting the data into train and test datasets. \n",
        "\n",
        "[MLFlow](https://mlflow.org/docs/latest/tracking.html) will be used to log the parameters and metrics during our pipeline run."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {data_prep_src_dir}/data_prep.py\n",
        "\n",
        "# Import necessary libraries and modules\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import logging\n",
        "import mlflow\n",
        "\n",
        "# Define the main function for the script\n",
        "def main():\n",
        "    \"\"\"Main function of the script.\"\"\"\n",
        "\n",
        "    # Define command-line arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--data\", type=str, help=\"path to input data\")\n",
        "    parser.add_argument(\"--test_train_ratio\", type=float, required=False, default=0.25)\n",
        "    parser.add_argument(\"--train_data\", type=str, help=\"path to train data\")\n",
        "    parser.add_argument(\"--test_data\", type=str, help=\"path to test data\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Start an MLflow run for logging\n",
        "    mlflow.start_run()\n",
        "\n",
        "    # Print the arguments passed to the script\n",
        "    print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
        "\n",
        "    # Load input data from the specified path\n",
        "    print(\"input data:\", args.data)\n",
        "    credit_df = pd.read_csv(args.data, header=1, index_col=0)\n",
        "\n",
        "    # Log metrics about the loaded dataset\n",
        "    mlflow.log_metric(\"num_samples\", credit_df.shape[0])\n",
        "    mlflow.log_metric(\"num_features\", credit_df.shape[1] - 1)\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    credit_train_df, credit_test_df = train_test_split(\n",
        "        credit_df,\n",
        "        test_size=args.test_train_ratio,\n",
        "    )\n",
        "\n",
        "    # Save the training and testing datasets to the specified paths\n",
        "    credit_train_df.to_csv(os.path.join(args.train_data, \"data.csv\"), index=False)\n",
        "    credit_test_df.to_csv(os.path.join(args.test_data, \"data.csv\"), index=False)\n",
        "\n",
        "    # End the MLflow run\n",
        "    mlflow.end_run()\n",
        "\n",
        "# Check if the script is being run directly\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./components/data_prep/data_prep.py\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "def-main",
        "gather": {
          "logged": 1697812041536
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary Azure Machine Learning libraries\n",
        "from azure.ai.ml import command\n",
        "from azure.ai.ml import Input, Output\n",
        "\n",
        "# Define a data preparation component using Azure Machine Learning commands\n",
        "data_prep_component = command(\n",
        "    name=\"data_prep_credit_defaults\",\n",
        "    display_name=\"Data preparation for training\",\n",
        "    description=\"Reads a .xl input, splits the input to train and test\",\n",
        "    inputs={\n",
        "        \"data\": Input(type=\"uri_folder\"),  # Define an input parameter for the component\n",
        "        \"test_train_ratio\": Input(type=\"number\"),  # Define another input parameter\n",
        "    },\n",
        "    outputs=dict(\n",
        "        train_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),  # Define an output parameter\n",
        "        test_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),  # Define another output parameter\n",
        "    ),\n",
        "    # Specify the source folder for the component's code\n",
        "    code=data_prep_src_dir,\n",
        "    # Define the command to be executed for this component\n",
        "    command=\"\"\"python data_prep.py \\\n",
        "            --data ${{inputs.data}} --test_train_ratio ${{inputs.test_train_ratio}} \\\n",
        "            --train_data ${{outputs.train_data}} --test_data ${{outputs.test_data}} \\\n",
        "            \"\"\",\n",
        "    # Specify the environment for running this component\n",
        "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "name": "data_prep_component",
        "gather": {
          "logged": 1697899282002
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the component to the workspace\n",
        "data_prep_component = ml_client.create_or_update(data_prep_component.component)\n",
        "\n",
        "# Create (register) the component in your workspace\n",
        "print(\n",
        "    f\"Component {data_prep_component.name} with Version {data_prep_component.version} is registered\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Component data_prep_credit_defaults with Version 2023-10-21-14-41-24-1346269 is registered\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1697899285437
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Create component 2: training (using yaml definition)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the 'os' module, which provides operating system-related functions.\n",
        "import os\n",
        "\n",
        "# Define a directory path for training source code, in this case, \"./components/train.\"\n",
        "train_src_dir = \"./components/train\"\n",
        "\n",
        "# Create a directory at the specified path if it doesn't already exist (exist_ok=True).\n",
        "os.makedirs(train_src_dir, exist_ok=True)\n"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "train_src_dir",
        "gather": {
          "logged": 1697899285525
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "Create the training script in the directory:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {train_src_dir}/train.py\n",
        "import argparse\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import os\n",
        "import pandas as pd\n",
        "import mlflow\n",
        "\n",
        "\n",
        "def select_first_file(path):\n",
        "    \"\"\"Selects first file in folder, use under assumption there is only one file in folder\n",
        "    Args:\n",
        "        path (str): path to directory or file to choose\n",
        "    Returns:\n",
        "        str: full path of selected file\n",
        "    \"\"\"\n",
        "    files = os.listdir(path)\n",
        "    return os.path.join(path, files[0])\n",
        "\n",
        "\n",
        "# Start Logging\n",
        "mlflow.start_run()\n",
        "\n",
        "# enable autologging\n",
        "mlflow.sklearn.autolog()\n",
        "\n",
        "os.makedirs(\"./outputs\", exist_ok=True)\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function of the script.\"\"\"\n",
        "\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--train_data\", type=str, help=\"path to train data\")\n",
        "    parser.add_argument(\"--test_data\", type=str, help=\"path to test data\")\n",
        "    parser.add_argument(\"--n_estimators\", required=False, default=100, type=int)\n",
        "    parser.add_argument(\"--learning_rate\", required=False, default=0.1, type=float)\n",
        "    parser.add_argument(\"--registered_model_name\", type=str, help=\"model name\")\n",
        "    parser.add_argument(\"--model\", type=str, help=\"path to model file\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # paths are mounted as folder, therefore, we are selecting the file from folder\n",
        "    train_df = pd.read_csv(select_first_file(args.train_data))\n",
        "\n",
        "    # Extracting the label column\n",
        "    y_train = train_df.pop(\"default payment next month\")\n",
        "\n",
        "    # convert the dataframe values to array\n",
        "    X_train = train_df.values\n",
        "\n",
        "    # paths are mounted as folder, therefore, we are selecting the file from folder\n",
        "    test_df = pd.read_csv(select_first_file(args.test_data))\n",
        "\n",
        "    # Extracting the label column\n",
        "    y_test = test_df.pop(\"default payment next month\")\n",
        "\n",
        "    # convert the dataframe values to array\n",
        "    X_test = test_df.values\n",
        "\n",
        "    print(f\"Training with data of shape {X_train.shape}\")\n",
        "\n",
        "    clf = GradientBoostingClassifier(\n",
        "        n_estimators=args.n_estimators, learning_rate=args.learning_rate\n",
        "    )\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Registering the model to the workspace\n",
        "    print(\"Registering the model via MLFlow\")\n",
        "    mlflow.sklearn.log_model(\n",
        "        sk_model=clf,\n",
        "        registered_model_name=args.registered_model_name,\n",
        "        artifact_path=args.registered_model_name,\n",
        "    )\n",
        "\n",
        "    # Saving the model to a file\n",
        "    mlflow.sklearn.save_model(\n",
        "        sk_model=clf,\n",
        "        path=os.path.join(args.model, \"trained_model\"),\n",
        "    )\n",
        "\n",
        "    # Stop Logging\n",
        "    mlflow.end_run()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./components/train/train.py\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "train.py",
        "gather": {
          "logged": 1697814039480
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        " Once the model is trained, the model file is saved and registered to the workspace. The registered model can now be used in inferencing endpoints.\n",
        "\n",
        "\n",
        "First, create the *yaml* file describing the component"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {train_src_dir}/train.yml\n",
        "# <component>\n",
        "name: train_credit_defaults_model\n",
        "display_name: Train Credit Defaults Model\n",
        "# version: 1 # Not specifying a version will automatically update the version\n",
        "type: command\n",
        "inputs:\n",
        "  train_data: \n",
        "    type: uri_folder\n",
        "  test_data: \n",
        "    type: uri_folder\n",
        "  learning_rate:\n",
        "    type: number     \n",
        "  registered_model_name:\n",
        "    type: string\n",
        "outputs:\n",
        "  model:\n",
        "    type: uri_folder\n",
        "code: .\n",
        "environment:\n",
        "  # for this step, we'll use an AzureML curate environment\n",
        "  azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:1\n",
        "command: >-\n",
        "  python train.py \n",
        "  --train_data ${{inputs.train_data}} \n",
        "  --test_data ${{inputs.test_data}} \n",
        "  --learning_rate ${{inputs.learning_rate}}\n",
        "  --registered_model_name ${{inputs.registered_model_name}} \n",
        "  --model ${{outputs.model}}\n",
        "# </component>\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./components/train/train.yml\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "train.yml"
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "Once the `yaml` file and the script are ready, we can create the component using `load_component()`. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the Component Package\n",
        "from azure.ai.ml import load_component\n",
        "\n",
        "# Loading the component from the yml file\n",
        "train_component = load_component(source=os.path.join(train_src_dir, \"train.yml\"))"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "train_component",
        "gather": {
          "logged": 1697899285799
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "Now create and register the component:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we register the component to the workspace\n",
        "train_component = ml_client.create_or_update(train_component)\n",
        "\n",
        "# Create (register) the component in your workspace\n",
        "print(\n",
        "    f\"Component {train_component.name} with Version {train_component.version} is registered\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\r\u001b[32mUploading train (0.0 MBs):   0%|          | 0/3723 [00:00<?, ?it/s]\r\u001b[32mUploading train (0.0 MBs): 100%|██████████| 3723/3723 [00:00<00:00, 38786.47it/s]\n\u001b[39m\n\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Component train_credit_defaults_model with Version 2023-10-21-14-41-28-5067901 is registered\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "update-train_component",
        "gather": {
          "logged": 1697899289122
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Create the pipeline from components\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# the dsl decorator tells the sdk that we are defining an Azure ML pipeline\n",
        "from azure.ai.ml import dsl, Input, Output\n",
        "\n",
        "\n",
        "@dsl.pipeline(\n",
        "    compute=\"serverless\",\n",
        "    description=\"E2E data_perp-train pipeline\",\n",
        ")\n",
        "def credit_defaults_pipeline(\n",
        "    pipeline_job_data_input,\n",
        "    pipeline_job_test_train_ratio,\n",
        "    pipeline_job_learning_rate,\n",
        "    pipeline_job_registered_model_name,\n",
        "):\n",
        "    # using data_prep_function like a python call with its own inputs\n",
        "    data_prep_job = data_prep_component(\n",
        "        data=pipeline_job_data_input,\n",
        "        test_train_ratio=pipeline_job_test_train_ratio,\n",
        "    )\n",
        "\n",
        "    # using train_func like a python call with its own inputs\n",
        "    train_job = train_component(\n",
        "        train_data=data_prep_job.outputs.train_data,  # note: using outputs from previous step\n",
        "        test_data=data_prep_job.outputs.test_data,  # note: using outputs from previous step\n",
        "        learning_rate=pipeline_job_learning_rate,  # note: using a pipeline input as parameter\n",
        "        registered_model_name=pipeline_job_registered_model_name,\n",
        "    )\n",
        "\n",
        "    # a pipeline returns a dictionary of outputs\n",
        "    # keys will code for the pipeline output identifier\n",
        "    return {\n",
        "        \"pipeline_job_train_data\": data_prep_job.outputs.train_data,\n",
        "        \"pipeline_job_test_data\": data_prep_job.outputs.test_data,\n",
        "    }"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "pipeline",
        "gather": {
          "logged": 1697899289384
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "Use pipeline definition to instantiate a pipeline with my dataset, split rate of choice and the name "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "registered_model_name = \"credit_defaults_model\"\n",
        "\n",
        "# Let's instantiate the pipeline with the parameters of our choice\n",
        "pipeline = credit_defaults_pipeline(\n",
        "    pipeline_job_data_input=Input(type=\"uri_file\", path=credit_data.path),\n",
        "    pipeline_job_test_train_ratio=0.25,\n",
        "    pipeline_job_learning_rate=0.05,\n",
        "    pipeline_job_registered_model_name=registered_model_name,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "registered_model_name",
        "gather": {
          "logged": 1697899289485
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Submit the job \n",
        "\n",
        "\n",
        "Once completed, the pipeline will register a model in the workspace as a result of training."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import webbrowser\n",
        "\n",
        "# submit the pipeline job\n",
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    pipeline,\n",
        "    # Project's name\n",
        "    experiment_name=\"e2e_registered_components\",\n",
        ")\n",
        "# open the pipeline in web browser\n",
        "webbrowser.open(pipeline_job.studio_url)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": "False"
          },
          "metadata": {}
        }
      ],
      "execution_count": 19,
      "metadata": {
        "name": "returned_job",
        "gather": {
          "logged": 1697899293239
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Deploy the model as an online endpoint"
      ],
      "metadata": {}
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Create a new online endpoint"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "\n",
        "# Creating a unique name for the endpoint\n",
        "online_endpoint_name = \"credit-endpoint-\" + str(uuid.uuid4())[:8]"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "online_endpoint_name",
        "gather": {
          "logged": 1697899293329
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import (\n",
        "    ManagedOnlineEndpoint,\n",
        "    ManagedOnlineDeployment,\n",
        "    Model,\n",
        "    Environment,\n",
        ")\n",
        "\n",
        "# create an online endpoint\n",
        "endpoint = ManagedOnlineEndpoint(\n",
        "    name=online_endpoint_name,\n",
        "    description=\"this is an online endpoint\",\n",
        "    auth_mode=\"key\",\n",
        "    tags={\n",
        "        \"training_dataset\": \"credit_defaults\",\n",
        "        \"model_type\": \"sklearn.GradientBoostingClassifier\",\n",
        "    },\n",
        ")\n",
        "\n",
        "endpoint_result = ml_client.begin_create_or_update(endpoint).result()\n",
        "\n",
        "print(\n",
        "    f\"Endpint {endpoint_result.name} provisioning state: {endpoint_result.provisioning_state}\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Endpint credit-endpoint-7ef6e030 provisioning state: Succeeded\n"
        }
      ],
      "execution_count": 21,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "endpoint",
        "gather": {
          "logged": 1697899389068
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "Once the endpoint is created, can be retrieved as below"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "endpoint = ml_client.online_endpoints.get(name=online_endpoint_name)\n",
        "\n",
        "print(\n",
        "    f'Endpint \"{endpoint.name}\" with provisioning state \"{endpoint.provisioning_state}\" is retrieved'\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Endpint \"credit-endpoint-7ef6e030\" with provisioning state \"Succeeded\" is retrieved\n"
        }
      ],
      "execution_count": 22,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "update-endpoint",
        "gather": {
          "logged": 1697899389882
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Deploy the model to the endpoint\n",
        "\n",
        "Once the endpoint is created, deploy the model with the entry script. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's pick the latest version of the model\n",
        "latest_model_version = max(\n",
        "    [int(m.version) for m in ml_client.models.list(name=registered_model_name)]\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "latest_model_version",
        "gather": {
          "logged": 1697899390290
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "Deploy the latest version of the model.  \n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# picking the model to deploy. Here we use the latest version of our registered model\n",
        "model = ml_client.models.get(name=registered_model_name, version=latest_model_version)\n",
        "\n",
        "\n",
        "# create an online deployment.\n",
        "blue_deployment = ManagedOnlineDeployment(\n",
        "    name=\"blue\",\n",
        "    endpoint_name=online_endpoint_name,\n",
        "    model=model,\n",
        "    instance_type=\"Standard_F4s_v2\",\n",
        "    instance_count=1,\n",
        ")\n",
        "\n",
        "blue_deployment_results = ml_client.online_deployments.begin_create_or_update(\n",
        "    blue_deployment\n",
        ").result()\n",
        "\n",
        "print(\n",
        "    f\"Deployment {blue_deployment_results.name} provisioning state: {blue_deployment_results.provisioning_state}\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Check: endpoint credit-endpoint-7ef6e030 exists\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "..............................................................................................Deployment blue provisioning state: Succeeded\n"
        }
      ],
      "execution_count": 24,
      "metadata": {
        "name": "model",
        "gather": {
          "logged": 1697899882927
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "### Test with a sample query\n",
        "\n",
        "Now that the model is deployed to the endpoint, we can run inference with it by creating a sample request file following the design expected in the run method in the score script."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "deploy_dir = \"./deploy\"\n",
        "os.makedirs(deploy_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 25,
      "metadata": {
        "name": "sample-request.json",
        "gather": {
          "logged": 1697899883089
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {deploy_dir}/sample-request.json\n",
        "{\n",
        "  \"input_data\": {\n",
        "    \"columns\": [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22],\n",
        "    \"index\": [0, 1],\n",
        "    \"data\": [\n",
        "            [20000,2,2,1,24,2,2,-1,-1,-2,-2,3913,3102,689,0,0,0,0,689,0,0,0,0],\n",
        "            [10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 10, 9, 8]\n",
        "        ]\n",
        "  }\n",
        "}"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing ./deploy/sample-request.json\n"
        }
      ],
      "execution_count": 26,
      "metadata": {
        "name": "write-sample-request"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test the blue deployment with some sample data\n",
        "ml_client.online_endpoints.invoke(\n",
        "    endpoint_name=online_endpoint_name,\n",
        "    request_file=\"./deploy/sample-request.json\",\n",
        "    deployment_name=\"blue\",\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 27,
          "data": {
            "text/plain": "'[1, 0]'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 27,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "ml_client.online_endpoints.invoke",
        "gather": {
          "logged": 1697899884488
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Clean up resources"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# ml_client.online_endpoints.begin_delete(name=online_endpoint_name)"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {
        "attributes": {
          "classes": [
            "python "
          ],
          "id": ""
        },
        "name": "ml_client.online_endpoints.begin_delete",
        "gather": {
          "logged": 1697899884596
        }
      }
    }
  ],
  "metadata": {
    "vscode": {
      "interpreter": {
        "hash": "78d13900fb236b5ce4c0b952de9b1270f1802c447ee20a989ea10ffd935aa825"
      }
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "celltoolbar": "Edit Metadata",
    "description": {
      "description": "Create production ML pipelines with Python SDK v2 in a Jupyter notebook"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
